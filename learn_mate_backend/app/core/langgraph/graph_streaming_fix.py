"""Enhanced streaming implementation for token-level streaming."""

from typing import AsyncGenerator
from app.core.logging import logger

async def get_token_stream_response(
    self, messages: list, session_id: str, user_id: str = None
) -> AsyncGenerator[str, None]:
    """Get token-level streaming response using astream_events.
    
    This implementation uses LangGraph's astream_events to capture individual
    tokens as they are generated by the LLM, providing true streaming behavior.
    """
    config = {"configurable": {"thread_id": session_id}}
    
    if self._graph is None:
        self._graph = await self.create_graph()
    
    token_count = 0
    accumulated_content = ""
    
    try:
        # Use astream_events for token-level streaming
        async for event in self._graph.astream_events(
            {"messages": self._dump_messages(messages), "session_id": session_id},
            config,
            version="v2"  # Use v2 for better event structure
        ):
            # Check for LLM streaming events
            if event["event"] == "on_chat_model_stream":
                # Extract token content from the event
                chunk_data = event.get("data", {})
                chunk = chunk_data.get("chunk", None)
                
                if chunk and hasattr(chunk, "content") and chunk.content:
                    token_count += 1
                    content = chunk.content
                    accumulated_content += content
                    
                    logger.debug(
                        "streaming_token",
                        session_id=session_id,
                        token_number=token_count,
                        token_content=repr(content),
                        accumulated_length=len(accumulated_content)
                    )
                    
                    yield content
            
            # Log other events for debugging
            elif event["event"] in ["on_chat_model_start", "on_chat_model_end"]:
                logger.debug(
                    "streaming_event",
                    session_id=session_id,
                    event_type=event["event"],
                    token_count=token_count
                )
        
        logger.info(
            "streaming_completed",
            session_id=session_id,
            total_tokens=token_count,
            total_length=len(accumulated_content)
        )
        
    except Exception as e:
        logger.error(
            "token_streaming_error",
            session_id=session_id,
            error=str(e),
            tokens_before_error=token_count
        )
        raise e


# Alternative implementation using direct LLM streaming
async def get_direct_stream_response(
    self, messages: list, session_id: str, user_id: str = None
) -> AsyncGenerator[str, None]:
    """Alternative implementation that bypasses LangGraph for direct LLM streaming.
    
    This can be used as a fallback if LangGraph's streaming doesn't work as expected.
    """
    # Prepare messages with system prompt
    from app.utils import prepare_messages
    from app.core.prompts import SYSTEM_PROMPT
    
    prepared_messages = prepare_messages(messages, self.llm, SYSTEM_PROMPT)
    
    try:
        # Stream directly from LLM
        async for chunk in self.llm.astream(prepared_messages):
            if chunk.content:
                yield chunk.content
                
    except Exception as e:
        logger.error(
            "direct_streaming_error",
            session_id=session_id,
            error=str(e)
        )
        raise e